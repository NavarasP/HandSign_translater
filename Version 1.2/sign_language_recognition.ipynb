{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System Configration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(gpu)\n",
    "    except RuntimeError as e:\n",
    "        print('error'+ e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weakproxy at 0x00000152A2E6E9D0 to Device at 0x00000152A2E564D0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.select_device(0)\n",
    "cuda.close()\n",
    "cuda.select_device(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language Recognition with RNN\n",
    "\n",
    "This notebook trains an RNN model to recognize sign language from landmark data extracted using MediaPipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the combined CSV file into a DataFrame\n",
    "# csv_file_path = 'D:/Projects/AIML/HandSign/Dataset_CSVs/transformed_data.csv'\n",
    "csv_file_path = 'D:/Projects/AIML/HandSign/Dataset_CSVs/keypoints_data.csv'\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   face_0_x  face_0_y  face_1_x  face_1_y  face_2_x  face_2_y  face_3_x  \\\n",
      "0  0.480987  0.600960  0.486844  0.549152  0.483475  0.566798  0.479284   \n",
      "1  0.515435  0.612552  0.521321  0.560277  0.517003  0.578228  0.511478   \n",
      "2  0.499053  0.592653  0.503803  0.538680  0.500637  0.558055  0.494142   \n",
      "3  0.437985  0.589782  0.439237  0.538740  0.438284  0.556461  0.429700   \n",
      "4  0.420540  0.584587  0.419919  0.535113  0.419445  0.551029  0.408256   \n",
      "\n",
      "   face_3_y  face_4_x  face_4_y  ...  pose_28_y  pose_29_x  pose_29_y  \\\n",
      "0  0.495447  0.488247  0.532376  ...   3.211675   0.699006   3.337574   \n",
      "1  0.506666  0.522563  0.543482  ...   3.176484   0.687595   3.314011   \n",
      "2  0.487191  0.504793  0.522018  ...   3.123261   0.685968   3.249211   \n",
      "3  0.488853  0.439693  0.522769  ...   3.069936   0.615897   3.201243   \n",
      "4  0.484223  0.419772  0.519081  ...   3.183950   0.663719   3.308161   \n",
      "\n",
      "   pose_30_x  pose_30_y  pose_31_x  pose_31_y  pose_32_x  pose_32_y  label  \n",
      "0   0.386315   3.310355   0.652768   3.440469   0.438355   3.428981      A  \n",
      "1   0.347050   3.274199   0.615969   3.411350   0.400396   3.408218      A  \n",
      "2   0.402161   3.224098   0.647203   3.359931   0.446049   3.337926      A  \n",
      "3   0.283825   3.157427   0.545426   3.312458   0.328650   3.307702      A  \n",
      "4   0.319413   3.274222   0.582834   3.420210   0.364315   3.421397      A  \n",
      "\n",
      "[5 rows x 1087 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175 entries, 0 to 174\n",
      "Columns: 1087 entries, face_0_x to label\n",
      "dtypes: float64(1086), object(1)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175, 1087)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_hand_columns = [col for col in df.columns if col.startswith('left_hand')]\n",
    "right_hand_columns = [col for col in df.columns if col.startswith('right_hand')]\n",
    "pose_columns = [col for col in df.columns if col.startswith('pose')]\n",
    "\n",
    "# Ensure the data is in the correct shape (number_of_samples, number_of_frames, number_of_features_per_frame)\n",
    "def reshape_data(df, columns, num_frames):\n",
    "    data = df[columns].values\n",
    "    num_samples = len(df) // num_frames\n",
    "    data = data.reshape(num_samples, num_frames, len(columns))\n",
    "    return data\n",
    "\n",
    "\n",
    "# Assuming num_frames is known\n",
    "num_frames = 1 # This should be the length of the time series\n",
    "\n",
    "left_hand_data = reshape_data(df, left_hand_columns, num_frames)\n",
    "right_hand_data = reshape_data(df, right_hand_columns, num_frames)\n",
    "pose_data = reshape_data(df, pose_columns, num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left hand data shape: (175, 1, 42)\n",
      "Right hand data shape: (175, 1, 42)\n",
      "Pose data shape: (175, 1, 66)\n"
     ]
    }
   ],
   "source": [
    "print(\"Left hand data shape:\", left_hand_data.shape)\n",
    "print(\"Right hand data shape:\", right_hand_data.shape)\n",
    "print(\"Pose data shape:\", pose_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the labels and convert them to one-hot encoding if necessary\n",
    "labels = df['label'].values[:len(df) // num_frames * num_frames]\n",
    "labels = labels.reshape(len(labels) // num_frames, num_frames)[:, 0]  # Assuming one label per sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "labels_onehot = onehot_encoder.fit_transform(labels_encoded.reshape(-1, 1))\n",
    "\n",
    "\n",
    "# with open('Models/label_encoder_word.pkl', 'wb') as file:\n",
    "#     pickle.dump(label_encoder, file)\n",
    "\n",
    "with open('Models/label_encoder_letter.pkl', 'wb') as file:\n",
    "    pickle.dump(label_encoder, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define input shapes for each branch\n",
    "hand_input_shape = (num_frames, len(right_hand_columns))\n",
    "pose_input_shape = (num_frames, len(pose_columns))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Multiply\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_multi_branch_model(hand_input_shape, pose_input_shape, number_of_classes, hand_units=50, pose_units=25):\n",
    "    \n",
    "    # Input layers for each set of features\n",
    "    left_hand_input = Input(shape=hand_input_shape)\n",
    "    right_hand_input = Input(shape=hand_input_shape)\n",
    "    pose_input = Input(shape=pose_input_shape)\n",
    "    \n",
    "    \n",
    "    # Left Hand branch\n",
    "    left_hand_lstm = LSTM(units=hand_units)(left_hand_input)\n",
    "    \n",
    "    # Right Hand branch\n",
    "    right_hand_lstm = LSTM(units=hand_units)(right_hand_input)\n",
    "    \n",
    "    # Pose branch\n",
    "    pose_lstm = LSTM(units=pose_units)(pose_input)\n",
    "    \n",
    "    \n",
    "    # Weighted combination of branches\n",
    "    left_hand_output = Multiply()([left_hand_lstm, Dense(1, activation='linear', use_bias=False)(left_hand_lstm)])\n",
    "    right_hand_output = Multiply()([right_hand_lstm, Dense(1, activation='linear', use_bias=False)(right_hand_lstm)])\n",
    "    pose_output = Multiply()([pose_lstm, Dense(1, activation='linear', use_bias=False)(pose_lstm)])\n",
    "    \n",
    "    # Concatenate the outputs\n",
    "    combined_output = Concatenate()([left_hand_output, right_hand_output, pose_output])\n",
    "    \n",
    "    # Final dense layer for classification\n",
    "    final_output = Dense(units=number_of_classes, activation='softmax')(combined_output)\n",
    "    \n",
    "    model = Model(inputs=[left_hand_input, right_hand_input, pose_input], outputs=final_output)\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = len(onehot_encoder.categories_[0])\n",
    "model = create_multi_branch_model(hand_input_shape, pose_input_shape,  number_of_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "X_left_train, X_left_val, X_right_train, X_right_val, X_pose_train, X_pose_val, y_train, y_val = train_test_split(\n",
    "    left_hand_data, right_hand_data, pose_data, labels_onehot, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/5 [=====>........................] - ETA: 33s - loss: 3.2589 - accuracy: 0.1562"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_left_train, X_right_train, X_pose_train], y_train, epochs=1000, batch_size=32,\n",
    "                    validation_data=([X_left_val, X_right_val, X_pose_val], y_val))\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_left_val, X_right_val, X_pose_val], y_val)\n",
    "print(f\"Validation Loss: {loss}, Validation Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model\n",
    "model.save('Models/sign_language_letter_model_2.h5')\n",
    "# model.save('Models/sign_language_word_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
