{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import joblib\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.initializers import Orthogonal\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {'Orthogonal': Orthogonal}\n",
    "loaded_model = tf.keras.models.load_model('Models/my_model.h5', custom_objects=custom_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def capture_video():\n",
    "    cap = cv2.VideoCapture(0)  # Open default camera (index 0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()  # Read frame from camera\n",
    "        cv2.imshow('Live Video', frame)  # Display frame\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks_mediapipe(frame):\n",
    "    with mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5) as hands:\n",
    "        with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "            with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "                # Convert BGR to RGB\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Process hand landmarks\n",
    "                hands_results = hands.process(frame_rgb)\n",
    "                left_hand_landmarks, right_hand_landmarks = [],[]\n",
    "                if hands_results.multi_hand_landmarks:\n",
    "                    for hand_landmarks, handedness in zip(hands_results.multi_hand_landmarks, hands_results.multi_handedness):\n",
    "                        if handedness.classification[0].label == 'Left':\n",
    "                            left_hand_landmarks = hand_landmarks\n",
    "                        elif handedness.classification[0].label == 'Right':\n",
    "                            right_hand_landmarks = hand_landmarks\n",
    "\n",
    "                # Process pose landmarks\n",
    "                pose_results = pose.process(frame_rgb)\n",
    "                pose_landmarks = pose_results.pose_landmarks\n",
    "\n",
    "\n",
    "                # Process face landmarks\n",
    "                face_results = face_mesh.process(frame_rgb)\n",
    "                face_landmarks = face_results.multi_face_landmarks\n",
    "\n",
    "                \n",
    "\n",
    "    return left_hand_landmarks, right_hand_landmarks, pose_landmarks, face_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_face_index = 467\n",
    "max_left_hand_index = 20\n",
    "max_right_hand_index = 20\n",
    "max_pose_index = 32\n",
    "\n",
    "face_columns = [f\"face_{i}\" for i in range(max_face_index + 1)]\n",
    "left_hand_columns = [f\"left_hand_{i}\" for i in range(max_left_hand_index + 1)]\n",
    "right_hand_columns = [f\"right_hand_{i}\" for i in range(max_right_hand_index + 1)]\n",
    "pose_columns = [f\"pose_{i}\" for i in range(max_pose_index + 1)]\n",
    "\n",
    "header =      [f\"{col}_{coord}\" for col in face_columns for coord in ['x', 'y']] + \\\n",
    "              [f\"{col}_{coord}\" for col in left_hand_columns for coord in ['x', 'y']] + \\\n",
    "              [f\"{col}_{coord}\" for col in right_hand_columns for coord in ['x', 'y']] + \\\n",
    "              [f\"{col}_{coord}\" for col in pose_columns for coord in ['x', 'y']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def landmarks_to_df(left_hand_landmarks, right_hand_landmarks, pose_landmarks, face_landmarks, header):\n",
    "    # Initialize dictionaries to store landmark data\n",
    "    landmarks_data = {}\n",
    "\n",
    "    # Process face landmarks\n",
    "    if face_landmarks:\n",
    "        for i, landmark_list in enumerate(face_landmarks):\n",
    "            for j, lm in enumerate(landmark_list.landmark):\n",
    "                landmarks_data[f\"face_{j}_x\"] = lm.x\n",
    "                landmarks_data[f\"face_{j}_y\"] = lm.y\n",
    "            # Fill missing face landmarks with zeros\n",
    "            for j in range(len(landmark_list.landmark), max_face_index + 1):\n",
    "                landmarks_data[f\"face_{j}_x\"] = 0.0\n",
    "                landmarks_data[f\"face_{j}_y\"] = 0.0\n",
    "    else:\n",
    "        # Fill all face landmarks with zeros if face_landmarks is None\n",
    "        for j in range(max_face_index + 1):\n",
    "            landmarks_data[f\"face_{j}_x\"] = 0.0\n",
    "            landmarks_data[f\"face_{j}_y\"] = 0.0\n",
    "\n",
    "    # Process left hand landmarks\n",
    "    if left_hand_landmarks:\n",
    "        for i, lm in enumerate(left_hand_landmarks.landmark):\n",
    "            landmarks_data[f\"left_hand_{i}_x\"] = lm.x\n",
    "            landmarks_data[f\"left_hand_{i}_y\"] = lm.y\n",
    "        # Fill missing left hand landmarks with zeros\n",
    "        for i in range(len(left_hand_landmarks.landmark), max_left_hand_index + 1):\n",
    "            landmarks_data[f\"left_hand_{i}_x\"] = 0.0\n",
    "            landmarks_data[f\"left_hand_{i}_y\"] = 0.0\n",
    "    else:\n",
    "        # Fill all left hand landmarks with zeros if left_hand_landmarks is None\n",
    "        for i in range(max_left_hand_index + 1):\n",
    "            landmarks_data[f\"left_hand_{i}_x\"] = 0.0\n",
    "            landmarks_data[f\"left_hand_{i}_y\"] = 0.0\n",
    "\n",
    "    # Process right hand landmarks\n",
    "    if right_hand_landmarks:\n",
    "        for i, lm in enumerate(right_hand_landmarks.landmark):\n",
    "            landmarks_data[f\"right_hand_{i}_x\"] = lm.x\n",
    "            landmarks_data[f\"right_hand_{i}_y\"] = lm.y\n",
    "        # Fill missing right hand landmarks with zeros\n",
    "        for i in range(len(right_hand_landmarks.landmark), max_right_hand_index + 1):\n",
    "            landmarks_data[f\"right_hand_{i}_x\"] = 0.0\n",
    "            landmarks_data[f\"right_hand_{i}_y\"] = 0.0\n",
    "    else:\n",
    "        # Fill all right hand landmarks with zeros if right_hand_landmarks is None\n",
    "        for i in range(max_right_hand_index + 1):\n",
    "            landmarks_data[f\"right_hand_{i}_x\"] = 0.0\n",
    "            landmarks_data[f\"right_hand_{i}_y\"] = 0.0\n",
    "\n",
    "    # Process pose landmarks\n",
    "    if pose_landmarks:\n",
    "        for i, lm in enumerate(pose_landmarks.landmark):\n",
    "            landmarks_data[f\"pose_{i}_x\"] = lm.x\n",
    "            landmarks_data[f\"pose_{i}_y\"] = lm.y\n",
    "        # Fill missing pose landmarks with zeros\n",
    "        for i in range(len(pose_landmarks.landmark), max_pose_index + 1):\n",
    "            landmarks_data[f\"pose_{i}_x\"] = 0.0\n",
    "            landmarks_data[f\"pose_{i}_y\"] = 0.0\n",
    "    else:\n",
    "        # Fill all pose landmarks with zeros if pose_landmarks is None\n",
    "        for i in range(max_pose_index + 1):\n",
    "            landmarks_data[f\"pose_{i}_x\"] = 0.0\n",
    "            landmarks_data[f\"pose_{i}_y\"] = 0.0\n",
    "\n",
    "    # Create DataFrame from extracted landmark data\n",
    "    df = pd.DataFrame([landmarks_data], columns=header)\n",
    "\n",
    "\n",
    "    return df.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frame(frame):\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Models/label_encoder.pkl', 'rb') as file:\n",
    "    label_encoder = pickle.load(file)\n",
    "\n",
    "# Load the saved OneHotEncoder\n",
    "with open('Models/onehot_encoder.pkl', 'rb') as file:\n",
    "    onehot_encoder = pickle.load(file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def live_sign_recognition(model,  header):\n",
    "    cap = cv2.VideoCapture(0)  # Open default camera\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)  # Flip the frame horizontally\n",
    "\n",
    "        # Extract landmarks using MediaPipe\n",
    "        left_hand, right_hand, pose, face = extract_landmarks_mediapipe(frame)\n",
    "\n",
    "        if left_hand or right_hand or pose or face:\n",
    "            # Preprocess landmarks\n",
    "            padded_landmarks = landmarks_to_df(left_hand, right_hand, pose, face, header)\n",
    "            padded_landmarks = np.expand_dims(padded_landmarks, axis=0)\n",
    "            padded_landmarks = np.expand_dims(padded_landmarks, axis=0)\n",
    "\n",
    "            # Debug: Print the shape of the input to the model\n",
    "\n",
    "            # Make prediction\n",
    "            prediction = model.predict(padded_landmarks)\n",
    "\n",
    "            # Debug: Print the raw prediction\n",
    "            # print(f\"Raw prediction: {prediction}\")\n",
    "\n",
    "            if not np.isnan(prediction).any():\n",
    "                # Convert prediction indices to sign labels using label encoder\n",
    "                # sign_labels = label_encoder.inverse_transform(np.argmax(prediction, axis=1))\n",
    "                # sign_label = sign_labels[0]  # Assuming single prediction per frame\n",
    "                predicted_class_index = np.argmax(prediction, axis=1)\n",
    "                predicted_class_label = label_encoder.inverse_transform(predicted_class_index)\n",
    "                \n",
    "                confidence = prediction[0, predicted_class_index][0]\n",
    "\n",
    "                # Print the class name if confidence is above 0.7\n",
    "                if confidence > 0.95:\n",
    "                    print(f\"Predicted sign: {predicted_class_label} with confidence {confidence:.2f}\")\n",
    "\n",
    "                # Display sign label on frame\n",
    "                cv2.putText(frame, str(predicted_class_label), (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                print(\"Prediction contains NaN values\")\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Frame', frame)\n",
    "\n",
    "        # Press 'q' to exit the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "# Assuming `model` is your trained model, `label_encoder` is the fitted label encoder, and `header` is the list of column names used in preprocessing\n",
    "# live_sign_recognition(model, label_encoder, header)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'putText'\n> Overload resolution failed:\n>  - Can't convert object to 'str' for 'text'\n>  - Can't convert object to 'str' for 'text'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlive_sign_recognition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 46\u001b[0m, in \u001b[0;36mlive_sign_recognition\u001b[1;34m(model, header)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted sign: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_class_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with confidence \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidence\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Display sign label on frame\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mputText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_class_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFONT_HERSHEY_SIMPLEX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction contains NaN values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'putText'\n> Overload resolution failed:\n>  - Can't convert object to 'str' for 'text'\n>  - Can't convert object to 'str' for 'text'\n"
     ]
    }
   ],
   "source": [
    "live_sign_recognition(loaded_model,header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mediapipe.framework.formats import landmark_pb2  # Import the protobuf definition\n",
    "\n",
    "# # Example list of NormalizedLandmarkList (replace with your actual data or how you receive it)\n",
    "# normalized_landmark_lists = [\n",
    "#     landmark_pb2.NormalizedLandmarkList(\n",
    "#         landmark=[\n",
    "#             landmark_pb2.NormalizedLandmark(x=0.1, y=0.2, z=0.3),\n",
    "#             landmark_pb2.NormalizedLandmark(x=0.4, y=0.5, z=0.6),\n",
    "#         ]\n",
    "#     ),\n",
    "#     landmark_pb2.NormalizedLandmarkList(\n",
    "#         landmark=[\n",
    "#             landmark_pb2.NormalizedLandmark(x=0.7, y=0.8, z=0.9),\n",
    "#             landmark_pb2.NormalizedLandmark(x=0.2, y=0.3, z=0.4),\n",
    "#         ]\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# # Iterate through each NormalizedLandmarkList\n",
    "# for landmark_list in normalized_landmark_lists:\n",
    "#     # Iterate through each NormalizedLandmark in the current list\n",
    "#     for landmark in landmark_list.landmark:\n",
    "#         print(f\"x: {landmark.x}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
